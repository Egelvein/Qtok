{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/petuch03/graph-rag-research/blob/master/tokenizers/self-attention-visualization.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeed29255010eb54"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from transformers import GemmaTokenizer, GemmaForCausalLM\n",
    "from sub_word_tokenization import tokenize_custom\n",
    "from attention_metrics import *\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# \n",
    "# notebook_login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:10:12.441567Z",
     "start_time": "2024-04-26T13:10:04.435592Z"
    }
   },
   "id": "e329b305d4472d0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "883ecd3f9aa1437f8c533904b5e57e6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27753f31ab9149328ccfef361c6cbe03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6a6aa98a25245f590f75607105bb642"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed68aa19089748a587d730ab501f31be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29f56e40b18d45ebb51ae2bfc5849673"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# configuration = GemmaConfig()\n",
    "model_version = 'google/gemma-2b-it'\n",
    "\n",
    "model = GemmaForCausalLM.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = GemmaTokenizer.from_pretrained(model_version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:10:41.012844Z",
     "start_time": "2024-04-26T13:10:12.442710Z"
    }
   },
   "id": "88bc75601c78acd9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GemmaModel is using GemmaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bob', '▁sent', '▁Alice', '▁a', '▁message', '▁about', '▁apples', '.']\n",
      "['B', 'o', 'b', '▁s', 'en', 't', '▁Al', 'ice', '▁', 'a', '▁mes', 'sage', '▁ab', 'out', '▁ap', 'ple', 's', '.']\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Bob sent Alice a message about apples.'\n",
    "comparison_pair = tokenize_custom(tokenizer, \"tokenizer.json\", \"Bob sent Alice a message about apples.\")\n",
    "print(comparison_pair.default_tokens)\n",
    "print(comparison_pair.sub_word_tokens)\n",
    "\n",
    "\n",
    "def compute_attention_pipeline(tokenizer, input_tokens) -> SimpleNamespace:\n",
    "    inputs = tokenizer.encode(input_tokens, return_tensors='pt')\n",
    "    # tokens = tokenizer.convert_ids_to_tokens(inputs[0])\n",
    "    outputs = model(inputs)\n",
    "    attention = torch.stack(outputs[-1], dim=0)\n",
    "    return SimpleNamespace(attention=attention, full_outputs=outputs)\n",
    "\n",
    "\n",
    "default_model_output = compute_attention_pipeline(tokenizer, comparison_pair.default_tokens)\n",
    "sub_word_model_output = compute_attention_pipeline(tokenizer, comparison_pair.sub_word_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:12:05.833311Z",
     "start_time": "2024-04-26T13:10:46.142730Z"
    }
   },
   "id": "2ff69243ad4ad553"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold-based Noise Metric - Source: tensor([0.8889, 0.7878, 0.7160, 0.6628, 0.6265, 0.5586, 0.5116, 0.5239, 0.4282]), Source mean: 0.6338306069374084\n",
      "Target: tensor([0.0293, 0.4660, 0.5278, 0.6073, 0.6852, 0.7515, 0.8333, 0.8750, 0.9290])Target mean: 0.6338306069374084\n",
      "Entropy-based Noise Metric: 2.1522603034973145\n",
      "Threshold-based Noise Metric - Source: tensor([0.9474, 0.9050, 0.8852, 0.8578, 0.8209, 0.8056, 0.7957, 0.7482, 0.7569,\n",
      "        0.7515, 0.7109, 0.7105, 0.7105, 0.6860, 0.6996, 0.6784, 0.7061, 0.6849,\n",
      "        0.6140]), Source mean: 0.7618498802185059\n",
      "Target: tensor([0.0523, 0.6583, 0.6981, 0.6528, 0.6857, 0.7482, 0.7460, 0.7485, 0.7379,\n",
      "        0.7778, 0.7865, 0.8527, 0.8198, 0.8706, 0.8757, 0.9097, 0.9243, 0.9565,\n",
      "        0.9737])Target mean: 0.7618498802185059\n",
      "Entropy-based Noise Metric: 2.921490430831909\n"
     ]
    }
   ],
   "source": [
    "show_all_metrics(default_model_output.attention, threshold=0.02)\n",
    "show_all_metrics(sub_word_model_output.attention, threshold=0.02)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:12:07.811572Z",
     "start_time": "2024-04-26T13:12:05.829866Z"
    }
   },
   "id": "a1132c88c44cf164"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['▁Bob']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = default_model_output.full_outputs.logits\n",
    "predicted_token_id = logits[:, -1, :].argmax(dim=-1)\n",
    "\n",
    "# Convert the predicted token ID to a token\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
    "predicted_token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T11:50:26.587485Z",
     "start_time": "2024-04-02T11:50:26.580410Z"
    }
   },
   "id": "72f67241db0b82f1"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def generate_sequence_from_tokens(tokenizer, input_tokens, max_length=50, num_return_sequences=1) -> list[str]:\n",
    "    inputs = tokenizer.encode(input_tokens, return_tensors='pt')\n",
    "    generated_sequences = model.generate(inputs, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "\n",
    "    generated_text = [tokenizer.decode(generated_sequence, skip_special_tokens=True) for generated_sequence in\n",
    "                      generated_sequences]\n",
    "\n",
    "    return generated_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:11:39.973463Z",
     "start_time": "2024-04-04T09:11:39.926961Z"
    }
   },
   "id": "158faf2850e21b9f"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "['Bob sent Alice a message about apples. Bob sent Alice a message about apples, but it was not the same message as the one he sent her. What happened?\\n\\nBob sent Alice a message about apples, but it was not the same message']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequence_from_tokens(tokenizer, comparison_pair.default_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:12:24.255249Z",
     "start_time": "2024-04-02T12:11:41.269706Z"
    }
   },
   "id": "88a0b54bf5b41ce"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "['Bob sent Alice a message about apples.\\n\\nSure, here\\'s the message about apples:\\n\\n\"Apples are a delicious fruit that is enjoyed by people of all ages. They are a good']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequence_from_tokens(tokenizer, comparison_pair.sub_word_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:16:10.989562Z",
     "start_time": "2024-04-02T12:14:31.238590Z"
    }
   },
   "id": "8011a6abd2354935"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def process_batch(batch, tokenizer=tokenizer, tokenizer_config: str = \"tokenizer.json\", threshold: float = 0.01):\n",
    "    batch_result = torch.empty(len(batch), 2, 2)\n",
    "    for idx, input_sequence in enumerate(batch):\n",
    "        comparison_pair = tokenize_custom(tokenizer, tokenizer_config, input_sequence)\n",
    "\n",
    "        default_model_attention = compute_attention_pipeline(tokenizer, comparison_pair.default_tokens).attention\n",
    "        sub_word_model_attention = compute_attention_pipeline(tokenizer, comparison_pair.sub_word_tokens).attention\n",
    "\n",
    "        default_threshold_metric = threshold_noise_metric(default_model_attention, threshold)\n",
    "        default_entropy_metric = entropy_based_noise_metric(default_model_attention)\n",
    "        batch_result[idx][0][0] = default_threshold_metric.source_noise_percentage_over_tokens\n",
    "        batch_result[idx][0][1] = default_entropy_metric\n",
    "\n",
    "        sub_word_threshold_metric = threshold_noise_metric(sub_word_model_attention, threshold)\n",
    "        sub_word_entropy_metric = entropy_based_noise_metric(sub_word_model_attention)\n",
    "        batch_result[idx][1][0] = sub_word_threshold_metric.source_noise_percentage_over_tokens\n",
    "        batch_result[idx][1][1] = sub_word_entropy_metric\n",
    "\n",
    "    return batch_result\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:28:42.666397Z",
     "start_time": "2024-04-02T12:28:42.656303Z"
    }
   },
   "id": "92d7516d323a5107"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "input_sequences = [\"Bob sent Alice a message about apples.\", \"Cat didn't cross the street because it was tired.\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:28:43.211235Z",
     "start_time": "2024-04-02T12:28:43.208475Z"
    }
   },
   "id": "fec6135a83a6ac48"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "batch_result = process_batch(input_sequences, tokenizer, \"tokenizer.json\", 0.02)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:29:04.209674Z",
     "start_time": "2024-04-02T12:28:43.734597Z"
    }
   },
   "id": "f1611ca0116de68f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.6338, 2.1523],\n         [0.7618, 2.9215]],\n\n        [[0.7086, 2.5323],\n         [0.7991, 3.2424]]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:29:04.234519Z",
     "start_time": "2024-04-02T12:29:04.216428Z"
    }
   },
   "id": "3cf66c1908151330"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def merge_sentences_from_file(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Strip leading and trailing whitespace, then remove the quotes\n",
    "            sentence = line.strip().strip('\"')\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "input_sequences = merge_sentences_from_file(\"test-sequences.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:08:14.467728Z",
     "start_time": "2024-04-04T09:08:14.462057Z"
    }
   },
   "id": "d1ee7ba5e3dfea34"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The cake was untouched because it was too sweet.', 'He missed the train because his alarm failed.', 'The plant wilted not from lack of water, but from too much sun.', \"She didn't answer the call, thinking it was a mistake.\", 'The dog barked at the shadow, mistaking it for an intruder.', 'The code wouldn’t compile, due to an unnoticed typo.', 'They returned the gift, finding it too practical than thoughtful.', 'The book remained on the shelf, deemed too advanced for beginners.', 'Bob sent Alice a message about apples.', \"Cat didn't cross the street because it was tired.\"]\n"
     ]
    }
   ],
   "source": [
    "print(input_sequences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:08:26.161135Z",
     "start_time": "2024-04-04T09:08:26.139985Z"
    }
   },
   "id": "4304ceca119e0e4b"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "comparison_pair_sequences = [tokenize_custom(tokenizer, \"tokenizer.json\", sequence) for sequence in input_sequences]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:24:45.594968Z",
     "start_time": "2024-04-04T09:24:43.392398Z"
    }
   },
   "id": "c67337f38ae112bf"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "default_tokens = [comparison_pair.default_tokens for comparison_pair in comparison_pair_sequences]\n",
    "sub_word_tokens = [comparison_pair.sub_word_tokens for comparison_pair in comparison_pair_sequences]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:27:22.015931Z",
     "start_time": "2024-04-04T09:27:22.002904Z"
    }
   },
   "id": "e331b72cef4dae2f"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'The cake was untouched because it was too sweet.'\n",
      "Generated: 'The cake was untouched because it was too sweet.\n",
      "\n",
      "The context suggests that the cake was too sweet, and that the speaker did not eat it.'\n",
      "Sub-word Generated: 'The cake was unto untouched because it was too sweet. The caker was so scared that he could not tak the cake out of the oven.'\n",
      "\n",
      "===========\n",
      "\n",
      "Input: 'He missed the train because his alarm failed.'\n",
      "Generated: 'He missed the train because his alarm failed. He had to wait for two hours before the next one arrived.\n",
      "\n",
      "The passage is about:\n",
      "\n",
      "A. A person who missed a train.\n",
      "B. A person who woke up late.\n",
      "'\n",
      "Sub-word Generated: 'He missed the train because his alarm failed.\n",
      "\n",
      "The train was due to arrive at the station at 10:00 AM, but it did not arrive until 11:0'\n",
      "\n",
      "===========\n",
      "\n",
      "Input: 'The plant wilted not from lack of water, but from too much sun.'\n",
      "Generated: 'The plant wilted not from lack of water, but from too much sun.\n",
      "\n",
      "Which is the cause of the wilting?\n",
      "\n",
      "A. Lack of water\n",
      "B. Too much water\n",
      "C. Too much sunlight\n",
      "\n",
      "The answer is B'\n",
      "Sub-word Generated: 'The plant wilted not from lack of water, but from too much sun.\n",
      "\n",
      "What is the answer?\n",
      "\n",
      "The plant will not thrive'\n",
      "\n",
      "===========\n",
      "\n",
      "Input: 'She didn't answer the call, thinking it was a mistake.'\n",
      "Generated: 'She didn't answer the call, thinking it was a mistake.\n",
      "\n",
      "The context does not provide any information about the person's phone or the call itself, so I cannot determine whether she answered or not.'\n",
      "Sub-word Generated: 'She didn't answer the call, thi thinking it was a mistake. She was supposed to answer the question, but she didn't.'\n",
      "\n",
      "===========\n",
      "\n",
      "Input: 'The dog barked at the shadow, mistaking it for an intruder.'\n",
      "Generated: 'The dog barked at the shadow, mistaking it for an intruder. The shadow moved across the yard, casting a long, dark shadow on the fence. The dog, unable to distinguish between friend and foe, lunged at the shadow, its'\n",
      "Sub-word Generated: 'The dog ba barked at the shadow, mistaking it for an intruder. The shadow was startled and ran away.\n",
      "\n",
      "What was the dog'\n",
      "\n",
      "===========\n",
      "\n",
      "Input: 'The code wouldn’t compile, due to an unnoticed typo.'\n",
      "Generated: 'The code wouldn’t compile, due to an unnoticed typo.\n",
      "\n",
      "```python\n",
      "def sum_of_squares(numbers):\n",
      "    sum = 0\n",
      "    for num in numbers:\n",
      "        if num != 0:\n",
      "            '\n",
      "Sub-word Generated: 'The code wouldn’t compile, due to an unnoticed typo.\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "The code would not compile due to an unnoticed typo. To fix this, we need'\n",
      "\n",
      "===========\n",
      "\n",
      "Input: 'They returned the gift, finding it too practical than thoughtful.'\n",
      "Generated: 'They returned the gift, finding it too practical than thoughtful.\n",
      "\n",
      "The phrase \"too practical\" means that something is not as meaningful or special as it could be. It is often used to describe a gift that is too common or something that is'\n",
      "Sub-word Generated: 'They returned the gift, finding it too practical than thou thoughtful.\n",
      "\n",
      "The gift was a small, round, wooden box. It was painted a vibrant blue, with intricate designs and'\n",
      "\n",
      "===========\n",
      "\n",
      "Input: 'The book remained on the shelf, deemed too advanced for beginners.'\n",
      "Generated: 'The book remained on the shelf, deemed too advanced for beginners.\n",
      "\n",
      "**Then, one day, a young student named Sarah decided to challenge herself.**\n",
      "\n",
      "**Determined to prove the book's worth, Sarah spent hours poring over the'\n",
      "Sub-word Generated: 'The book remained on the shelf, deemed too advanced for beginners.\n",
      "\n",
      "The book was written by a professional author, and it was intended to help beginners learn the basics'\n",
      "\n",
      "===========\n",
      "\n",
      "Input: 'Bob sent Alice a message about apples.'\n",
      "Generated: 'Bob sent Alice a message about apples. Bob sent Alice a message about apples, but it was not the same message as the one he sent her. What happened?\n",
      "\n",
      "Bob sent Alice a message about apples, but it was not the same message'\n",
      "Sub-word Generated: 'Bob sent Alice a message about apples.\n",
      "\n",
      "Sure, here's the message about apples:\n",
      "\n",
      "\"Apples are a delicious fruit that is enjoyed by people of all ages. They are a good'\n",
      "\n",
      "===========\n",
      "\n",
      "Input: 'Cat didn't cross the street because it was tired.'\n",
      "Generated: 'Cat didn't cross the street because it was tired.\n",
      "\n",
      "The correct answer is: Cat did not cross the street because it was tired.'\n",
      "Sub-word Generated: 'Cat didn't cross the street because it was tired.\n",
      "\n",
      "The correct answer is: Because it was tired.\n",
      "\n",
      "The phrase \"because it was tired\" is used to indicate'\n",
      "\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "default_generated_sequences_list = [generate_sequence_from_tokens(tokenizer, default) for default in default_tokens]\n",
    "sub_word_generated_sequences_list = [generate_sequence_from_tokens(tokenizer, sub_word) for sub_word in sub_word_tokens]\n",
    "\n",
    "# Print the generated sequences\n",
    "for i in range(0, len(default_generated_sequences_list)):\n",
    "    print(f\"Input: '{input_sequences[i]}'\")\n",
    "    for sequence in default_generated_sequences_list[i]:\n",
    "        print(f\"Generated: '{sequence}'\")\n",
    "    for sub_words_sequence in sub_word_generated_sequences_list[i]:\n",
    "        print(f\"Sub-word Generated: '{sub_words_sequence}'\")\n",
    "    print(\"\\n===========\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:39:59.615158Z",
     "start_time": "2024-04-04T09:29:41.412379Z"
    }
   },
   "id": "b56c29f4084a5657"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at BAAI/bge-large-en-v1.5 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'GemmaTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not a string",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer\n\u001B[1;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBAAI/bge-large-en-v1.5\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mGemmaTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mBAAI/bge-large-en-v1.5\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# comparison_pair_sequences = [tokenize_custom(tokenizer, \"tokenizer-BAAI:bge-large-en-v1.5.json\", sequence) for sequence in input_sequences]\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# default_tokens = [comparison_pair.default_tokens for comparison_pair in comparison_pair_sequences]\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# sub_word_tokens = [comparison_pair.sub_word_tokens for comparison_pair in comparison_pair_sequences]\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m#         print(f\"Sub-word Generated: '{sub_words_sequence}'\")\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m#     print(\"\\n===========\\n\")\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:2048\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2045\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2046\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2048\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2049\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2050\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2051\u001B[0m \u001B[43m    \u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2052\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2053\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2054\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2055\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2056\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2057\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_is_local\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_local\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2058\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2059\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2060\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:2287\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2285\u001B[0m \u001B[38;5;66;03m# Instantiate the tokenizer.\u001B[39;00m\n\u001B[1;32m   2286\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2287\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2288\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m   2289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[1;32m   2290\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to load vocabulary from file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2291\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2292\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/gemma/tokenization_gemma.py:114\u001B[0m, in \u001B[0;36mGemmaTokenizer.__init__\u001B[0;34m(self, vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces, use_default_system_prompt, spaces_between_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_default_system_prompt \u001B[38;5;241m=\u001B[39m use_default_system_prompt\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msp_model \u001B[38;5;241m=\u001B[39m spm\u001B[38;5;241m.\u001B[39mSentencePieceProcessor(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msp_model_kwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msp_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    117\u001B[0m     bos_token\u001B[38;5;241m=\u001B[39mbos_token,\n\u001B[1;32m    118\u001B[0m     eos_token\u001B[38;5;241m=\u001B[39meos_token,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    128\u001B[0m )\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/sentencepiece/__init__.py:905\u001B[0m, in \u001B[0;36mSentencePieceProcessor.Load\u001B[0;34m(self, model_file, model_proto)\u001B[0m\n\u001B[1;32m    903\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_proto:\n\u001B[1;32m    904\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLoadFromSerializedProto(model_proto)\n\u001B[0;32m--> 905\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadFromFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/sentencepiece/__init__.py:310\u001B[0m, in \u001B[0;36mSentencePieceProcessor.LoadFromFile\u001B[0;34m(self, arg)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mLoadFromFile\u001B[39m(\u001B[38;5;28mself\u001B[39m, arg):\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_sentencepiece\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSentencePieceProcessor_LoadFromFile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: not a string"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "tokenizer = GemmaTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# comparison_pair_sequences = [tokenize_custom(tokenizer, \"tokenizer-BAAI:bge-large-en-v1.5.json\", sequence) for sequence in input_sequences]\n",
    "# default_tokens = [comparison_pair.default_tokens for comparison_pair in comparison_pair_sequences]\n",
    "# sub_word_tokens = [comparison_pair.sub_word_tokens for comparison_pair in comparison_pair_sequences]\n",
    "# default_generated_sequences_list = [generate_sequence_from_tokens(tokenizer, default) for default in default_tokens]\n",
    "# sub_word_generated_sequences_list = [generate_sequence_from_tokens(tokenizer, sub_word) for sub_word in sub_word_tokens]\n",
    "# \n",
    "# # Print the generated sequences\n",
    "# for i in range(0, len(default_generated_sequences_list)):\n",
    "#     print(f\"Input: '{input_sequences[i]}'\")\n",
    "#     for sequence in default_generated_sequences_list[i]:\n",
    "#         print(f\"Generated: '{sequence}'\")\n",
    "#     for sub_words_sequence in sub_word_generated_sequences_list[i]:\n",
    "#         print(f\"Sub-word Generated: '{sub_words_sequence}'\")\n",
    "#     print(\"\\n===========\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:21:13.402181Z",
     "start_time": "2024-04-04T10:21:10.231736Z"
    }
   },
   "id": "c53ebcc5d801b695"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "comparison_pair_sequences = [tokenize_custom(tokenizer, \"tokenizer-BAAI:bge-large-en-v1.5.json\", sequence) for sequence in input_sequences]\n",
    "default_tokens = [comparison_pair.default_tokens for comparison_pair in comparison_pair_sequences]\n",
    "sub_word_tokens = [comparison_pair.sub_word_tokens for comparison_pair in comparison_pair_sequences]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:11:20.381120Z",
     "start_time": "2024-04-04T10:11:20.266912Z"
    }
   },
   "id": "87646a90954e1ea"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'token': 'list' object cannot be converted to 'PyString'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_tokens_to_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdefault_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m generated_sequences \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(inputs, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, num_return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      4\u001B[0m generated_text \u001B[38;5;241m=\u001B[39m [tokenizer\u001B[38;5;241m.\u001B[39mdecode(generated_sequence, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m generated_sequence \u001B[38;5;129;01min\u001B[39;00m\n\u001B[1;32m      5\u001B[0m                       generated_sequences]\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_fast.py:331\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast.convert_tokens_to_ids\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tokens, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_token_to_id_with_added_voc(tokens)\n\u001B[0;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_token_to_id_with_added_voc(token) \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m tokens]\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_fast.py:331\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tokens, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_token_to_id_with_added_voc(tokens)\n\u001B[0;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_token_to_id_with_added_voc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m tokens]\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_fast.py:334\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._convert_token_to_id_with_added_voc\u001B[0;34m(self, token)\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_convert_token_to_id_with_added_voc\u001B[39m(\u001B[38;5;28mself\u001B[39m, token: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m--> 334\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoken_to_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    336\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munk_token_id\n",
      "\u001B[0;31mTypeError\u001B[0m: argument 'token': 'list' object cannot be converted to 'PyString'"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(default_tokens)\n",
    "generated_sequences = model.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "\n",
    "generated_text = [tokenizer.decode(generated_sequence, skip_special_tokens=True) for generated_sequence in\n",
    "                      generated_sequences]\n",
    "generated_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:16:01.177682Z",
     "start_time": "2024-04-04T10:16:01.130325Z"
    }
   },
   "id": "4de13b99d2243043"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "84eb1706c4f9a4b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
